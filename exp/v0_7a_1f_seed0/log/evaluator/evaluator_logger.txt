[2024-11-15 01:30:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 250.0, current episode: 1
[2024-11-15 01:30:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 250.0, current episode: 2
[2024-11-15 01:30:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 250.0, current episode: 3
[2024-11-15 01:30:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 250.0, current episode: 4
[2024-11-15 01:30:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 250.0, current episode: 5
[2024-11-15 01:30:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 5 finish episode, final reward: 250.0, current episode: 6
[2024-11-15 01:30:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 6 finish episode, final reward: 250.0, current episode: 7
[2024-11-15 01:30:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 7 finish episode, final reward: 250.0, current episode: 8
[2024-11-15 01:30:44][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+---------------------+---------------+---------------+
| Name  | train_iter | ckpt_name           | episode_count | envstep_count |
+-------+------------+---------------------+---------------+---------------+
| Value | 0.000000   | iteration_0.pth.tar | 8.000000      | 240.000000    |
+-------+------------+---------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 30.000000               | 0.296775      | 808.694474          | 26.956482            |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 250.000000  | 0.000000   | 250.000000 | 250.000000 |
+-------+-------------+------------+------------+------------+


[2024-11-15 01:31:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 231.0, current episode: 1
[2024-11-15 01:31:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 231.0, current episode: 2
[2024-11-15 01:31:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 231.0, current episode: 3
[2024-11-15 01:31:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 231.0, current episode: 4
[2024-11-15 01:31:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 231.0, current episode: 5
[2024-11-15 01:31:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 5 finish episode, final reward: 231.0, current episode: 6
[2024-11-15 01:31:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 6 finish episode, final reward: 231.0, current episode: 7
[2024-11-15 01:31:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 7 finish episode, final reward: 231.0, current episode: 8
[2024-11-15 01:31:47][interaction_serial_evaluator.py:279][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2000.000000 | iteration_2000.pth.tar | 8.000000      | 320.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 40.000000               | 0.387805      | 825.156498          | 20.628912            |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 231.000000  | 0.000000   | 231.000000 | 231.000000 |
+-------+-------------+------------+------------+------------+


[2024-11-15 01:33:10][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: -415.0, current episode: 1
[2024-11-15 01:33:10][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: -415.0, current episode: 2
[2024-11-15 01:33:10][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: -415.0, current episode: 3
[2024-11-15 01:33:10][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: -415.0, current episode: 4
[2024-11-15 01:33:10][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: -415.0, current episode: 5
[2024-11-15 01:33:10][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 5 finish episode, final reward: -415.0, current episode: 6
[2024-11-15 01:33:10][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 6 finish episode, final reward: -415.0, current episode: 7
[2024-11-15 01:33:10][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 7 finish episode, final reward: -415.0, current episode: 8
[2024-11-15 01:33:10][interaction_serial_evaluator.py:279][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4000.000000 | iteration_4000.pth.tar | 8.000000      | 16040.000000  |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 2005.000000             | 18.963521     | 845.834463          | 0.421863             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+-------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min  |
+-------+-------------+------------+-------------+-------------+
| Value | -415.000000 | 0.000000   | -415.000000 | -415.000000 |
+-------+-------------+------------+-------------+-------------+


[2024-11-15 01:34:16][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 230.0, current episode: 1
[2024-11-15 01:34:16][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 230.0, current episode: 2
[2024-11-15 01:34:16][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 230.0, current episode: 3
[2024-11-15 01:34:16][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 230.0, current episode: 4
[2024-11-15 01:34:16][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 230.0, current episode: 5
[2024-11-15 01:34:16][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 5 finish episode, final reward: 230.0, current episode: 6
[2024-11-15 01:34:16][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 6 finish episode, final reward: 230.0, current episode: 7
[2024-11-15 01:34:16][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 7 finish episode, final reward: 230.0, current episode: 8
[2024-11-15 01:34:16][interaction_serial_evaluator.py:279][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6000.000000 | iteration_6000.pth.tar | 8.000000      | 320.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 40.000000               | 0.410503      | 779.531170          | 19.488279            |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 230.000000  | 0.000000   | 230.000000 | 230.000000 |
+-------+-------------+------------+------------+------------+


[2024-11-15 01:35:24][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 252.0, current episode: 1
[2024-11-15 01:35:24][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 252.0, current episode: 2
[2024-11-15 01:35:24][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 252.0, current episode: 3
[2024-11-15 01:35:24][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 252.0, current episode: 4
[2024-11-15 01:35:24][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 252.0, current episode: 5
[2024-11-15 01:35:24][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 5 finish episode, final reward: 252.0, current episode: 6
[2024-11-15 01:35:24][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 6 finish episode, final reward: 252.0, current episode: 7
[2024-11-15 01:35:24][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 7 finish episode, final reward: 252.0, current episode: 8
[2024-11-15 01:35:24][interaction_serial_evaluator.py:279][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 8000.000000 | iteration_8000.pth.tar | 8.000000      | 216.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 27.000000               | 0.458061      | 471.552936          | 17.464924            |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 252.000000  | 0.000000   | 252.000000 | 252.000000 |
+-------+-------------+------------+------------+------------+


[2024-11-15 01:36:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 252.0, current episode: 1
[2024-11-15 01:36:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 252.0, current episode: 2
[2024-11-15 01:36:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 252.0, current episode: 3
[2024-11-15 01:36:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 252.0, current episode: 4
[2024-11-15 01:36:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 252.0, current episode: 5
[2024-11-15 01:36:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 5 finish episode, final reward: 252.0, current episode: 6
[2024-11-15 01:36:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 6 finish episode, final reward: 252.0, current episode: 7
[2024-11-15 01:36:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 7 finish episode, final reward: 252.0, current episode: 8
[2024-11-15 01:36:38][interaction_serial_evaluator.py:279][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 10000.000000 | iteration_10000.pth.tar | 8.000000      | 216.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 27.000000               | 0.314717      | 686.330256          | 25.419639            |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 252.000000  | 0.000000   | 252.000000 | 252.000000 |
+-------+-------------+------------+------------+------------+


[2024-11-15 01:37:58][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 231.0, current episode: 1
[2024-11-15 01:37:58][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 231.0, current episode: 2
[2024-11-15 01:37:58][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 231.0, current episode: 3
[2024-11-15 01:37:58][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 231.0, current episode: 4
[2024-11-15 01:37:58][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 231.0, current episode: 5
[2024-11-15 01:37:58][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 5 finish episode, final reward: 231.0, current episode: 6
[2024-11-15 01:37:58][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 6 finish episode, final reward: 231.0, current episode: 7
[2024-11-15 01:37:58][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 7 finish episode, final reward: 231.0, current episode: 8
[2024-11-15 01:37:58][interaction_serial_evaluator.py:279][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 12000.000000 | iteration_12000.pth.tar | 8.000000      | 320.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 40.000000               | 0.463114      | 690.975253          | 17.274381            |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 231.000000  | 0.000000   | 231.000000 | 231.000000 |
+-------+-------------+------------+------------+------------+


